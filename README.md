# Kaggle_Spaceship_Titanic

This repository contains a complete machine learning pipeline for the **Spaceship Titanic Kaggle competition**, which predicts whether a passenger was transported to another dimension.


## ğŸ“ Project Structure

```
ğŸ“¦ Spaceship_Titanic/
â”œâ”€â”€ ğŸ“Š Exploratory Data Analysis
â”œâ”€â”€ ğŸ”§ Preprocessing & Feature Engineering
â”œâ”€â”€ ğŸ¤– Modeling using CatBoost
â”œâ”€â”€ ğŸ“ˆ Evaluation & Explainable AI(XAI)
â”œâ”€â”€ ğŸ“ data/ (linked to Kaggle)
â””â”€â”€ ğŸ“„ Spaceship_Titanic_18.ipynb
```



## ğŸ“Œ Objective

Predict the `Transported` status of passengers aboard the Spaceship Titanic based on their demographic information, travel records, and onboard spending behavior.


## ğŸ§ª Dataset Overview

- Source: [Kaggle - Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)
- Files: `train.csv`, `test.csv`
- Features:
  - Categorical: `HomePlanet`, `CryoSleep`, `Cabin`, `Destination`, `VIP`
  - Numerical: `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`
  - Target: `Transported` (True/False)


## ğŸ“Š EDA Highlights

- Visualized missing data with `missingno`
- Explored relationships between Age, Spending, and Transported status
- Outliers and skewed spending distributions were identified


## ğŸ”§ Preprocessing

- Handled missing values
- Encoded categorical variables using `TargetEncoder`
- Scaled numerical features using `StandardScaler`


## ğŸ—ï¸ Feature Engineering

- Generated interaction terms using `PolynomialFeatures`
- Aggregated total spending (`RoomService`, `Spa`, etc.)
- Suggested extraction from `Cabin` into `Deck`, `Number`, `Side`

## ğŸ§ª Model Selection Strategy

During the initial modeling phase, we experimented with multiple classification algorithms including:

- **Random Forest**
- **XGBoost**
- **CatBoost**

After comparative analysis on early validation sets, **CatBoostClassifier** demonstrated the most promising results in terms of both accuracy and robustness to categorical data. As a result, we focused further experimentation, feature engineering, and tuning on CatBoost.

> ğŸ” *Note:* The implementation and training code for **Random Forest** and **XGBoost** are maintained in separate branches of this repository for reference and reproducibility.
"""

## ğŸ¤– Modeling

- Model: `CatBoostClassifier` 
- Strategy: Stratified K-Fold Cross-Validation
- Evaluation: Confusion Matrix & Classification Report


## ğŸ“ˆ Performance Analysis

- Accurate and stable results across folds
- Identified key predictive features
- Achieve a Kaggle competition score of 0.80921


## ğŸ“Œ Requirements

This project was developed and run in **Google Colab**, which provides a ready-to-use Python environment.

Additional packages were installed in the notebook itself:

```python
!pip install catboost
!pip install category_encoders
```
Most other libraries like pandas, numpy, matplotlib, seaborn, scikit-learn, and missingno are pre-installed in Colab.

â¡ï¸ No local setup is required â€” just open the notebook in Colab and run all cells!

## ğŸ“š Author

Developed by Group 18( Space Predators) for academic submission.  
Built for learning, exploration, and Kaggle experimentation.

---

## ğŸ“ License

This project is for educational purposes only and follows the [Kaggle rules of competition].
